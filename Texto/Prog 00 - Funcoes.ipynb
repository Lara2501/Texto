{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programa com Funcoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caminhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Folder Inicial\n",
    "path = os.getcwd()\n",
    "\n",
    "#Subpastas\n",
    "pathin = path + '\\\\Entrada\\\\'\n",
    "pathfixo = path + '\\\\Fixo\\\\'\n",
    "pathout = path + '\\\\Saida\\\\'\n",
    "pathparcial = path + '\\\\Parcial\\\\'\n",
    "pathaux = path + '\\\\Auxiliar\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import unidecode\n",
    "from unicodedata import normalize\n",
    "\n",
    "import pygtrie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcoes \"Fixas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "#Removendo acentos\n",
    "########################################################################################\n",
    "\n",
    "rem_acentos = lambda x: normalize('NFKD', x).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "########################################################################################\n",
    "#Funcao para otimizar Stem\n",
    "########################################################################################\n",
    "\n",
    "def Tokenize(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = nltk.word_tokenizer(sentence)\n",
    "    return sentence\n",
    "\n",
    "def Stemming(sentence):\n",
    "    stemmer = RSLPStemmer()\n",
    "    phrase = []\n",
    "    \n",
    "    for word in sentence:\n",
    "            phrase.append(stemmer.stem(word.lower()))\n",
    "    \n",
    "    return phrase\n",
    "\n",
    "########\n",
    "# Stem #\n",
    "########\n",
    "def stem(col):\n",
    "    \n",
    "    #Substitui oes por ao (por causa de palavras: cartoes para cartao, por exemplo)\n",
    "    #Isso porque removemos os acentos antes\n",
    "    col = col.apply(lambda x: re.sub(r'oes\\b', 'ao', x))\n",
    "    \n",
    "    #Tokenize\n",
    "    col = col. apply(lambda x: Tokenize(x))\n",
    "    \n",
    "    #Stem\n",
    "    col = col.apply(lambda x: Stemming(x))\n",
    "    \n",
    "    #Back to Sentence\n",
    "    col = col.apply(lambda L: \"\".join(str(x) for x in L))\n",
    "    \n",
    "    return col\n",
    "\n",
    "########################################################################################\n",
    "#Removendo StoWords com base em uma arvore\n",
    "########################################################################################\n",
    "\n",
    "def limpa_nomes(text, wordsremove):\n",
    "    \n",
    "    tree = pygtrie.StringTrie()\n",
    "    \n",
    "    #Para cada palavra a ser removida criar um par com:\n",
    "    #a palavra que deve ser substituida e o valor a substituir\n",
    "    for word in wordsremove:\n",
    "        tree[word] = ''\n",
    "        \n",
    "    string = ''\n",
    "    for token in text.split():\n",
    "        if token not in tree:\n",
    "            string = string + token + ''\n",
    "            \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcao de Preprocessamento (mudar conforme a base de dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcess(data, textcol, maxchar, minchar, wordstoremove):\n",
    "    \n",
    "    #sw = StopWords('portuguese')\n",
    "    #sw = StopWords('english')\n",
    "    \n",
    "    #Transformando tudo em minuscula e removendo acento\n",
    "    data[textcol] = data[textcol].apply(lambda x: x.lower()).apply(rem_acentos)\n",
    "       \n",
    "    print('Fim da Remocao de Acentos')\n",
    "    print(strftime('%Y-%m-%d %H:%M:%S', gmtime()))\n",
    "    \n",
    "    #Tirando Mascara (caso tenhha), cumprimentos, e expressoes que atrapalham (bom dia, sendo assim)\n",
    "    #data[textcol] = data[textcol].apply(lambda x: re.sub(r'<div>', ' ', x))\n",
    "    \n",
    "    #Removendo nomes\n",
    "    data[textcol] = data[textcol].apply(lambda x: limpa_nomes(x, wordstoremove))\n",
    "    \n",
    "    print('Fim da Remocao de Nomes')\n",
    "    print(strftime('%Y-%m-%d %H:%M:%S', gmtime()))\n",
    "    \n",
    "    #Removendo \\n e \\t\n",
    "    vec = [r'\\n', r'\\t']\n",
    "    for i in range(0, len(vec)):\n",
    "        data[textcol] = data[textcol].apply(lambda x: x.replace(vec[i], ''))\n",
    "        \n",
    "    #Removendo pontuacao, numeros.. ficam apenas as letras minusculas\n",
    "    data[textcol] = data[textcol].apply(lambda x: re.sub(r'[^a-z]', ' ', x))\n",
    "    \n",
    "    #Caso queira pegar o texto depois de um certo ponto ou ate um certo ponto\n",
    "    #sep = 'Mascara Inicio'\n",
    "    #data[textcol] = data[textcol].apply(lambda x: x.split(sep, 1)[-1])\n",
    "    #sep = 'Mascara Fim'\n",
    "    #data[textcol] = data[textcol].apply(lambda x: x.split(sep, 1)[0])\n",
    "    \n",
    "    #Removendo os @User\n",
    "    data[textcol] = data[textcol].apply(lambda txt: ' '.join(word for word in txt.split(' ') if not word.startswith('@')))   \n",
    "    \n",
    "    #Removendo palavras com poucas e muitas letras\n",
    "    data[textcol] = data[textcol].apply(lambda x: re.sub(r'\\b[a-z]{1,}' + str(minchar - 1) + r'}\\b', ' ', x))\n",
    "    data[textcol] = data[textcol].apply(lambda x: re.sub(r'\\b[a-z]{' + str(maxchar + 1) + r',300}\\b', ' ', x))\n",
    "      \n",
    "    #Tirar espacos duplos e espacos de comeco e fim de celula\n",
    "    data[textcol] = data[textcol].apply(lambda x: re.sub(r' +', ' ', x))\n",
    "    data[textcol] = data[textcol].apply(lambda x: x.strip())\n",
    "    \n",
    "    print('Fim da Remocao de Palavras Curtas e Longas')\n",
    "    print(strftime('%Y-%m-%d %H:%M:%S', gmtime()))\n",
    "    \n",
    "    ####################################################################################\n",
    "    #Criacao de Variaveis\n",
    "    ####################################################################################\n",
    "    \n",
    "    #########################################\n",
    "    #Criacao de Variaveis: Tamanho de texto\n",
    "    #########################################\n",
    "    \n",
    "    #Total de Caracteres\n",
    "    data['length'] = data[textcol].apply(lambda x: len(x))\n",
    "    #Total de Palavras\n",
    "    data['words'] = data[textcol].apply(lambda x: len(x.split(' ')))\n",
    "    \n",
    "    #Numero Medio de Caracters das palavras\n",
    "    data['avg_word_length'] = data[textcol].apply(lambda x: np.mean([len(t) for t in x.split(' ')]))\n",
    "\n",
    "    #Numero Minimo de Caracters das palavras\n",
    "    data['min_word_length'] = data[textcol].apply(lambda x: np.min([len(t) for t in x.split(' ')]))\n",
    "     \n",
    "    #Numero Maximo de Caracters das palavras\n",
    "    data['max_word_length'] = data[textcol].apply(lambda x: np.max([len(t) for t in x.split(' ')]))\n",
    "                                                      \n",
    "    print('Fim da Contagem de Palavras')\n",
    "    print(strftime('%Y-%m-%d %H:%M:%S', gmtime()))\n",
    "                                                  \n",
    "    #########################################\n",
    "    #Criacao de Variaveis: O que contem\n",
    "    #########################################\n",
    "    \n",
    "    #words0 = ['ccf', 'oi']\n",
    "    #words1 = '|'.join(words0)\n",
    "    #data['DummyCCF'] = np.where(data[textcol].str.contains(words1, na = False, case = True), 1, 0)\n",
    "                      \n",
    "    ####################################################################################\n",
    "    #Stem\n",
    "    ####################################################################################\n",
    "                      \n",
    "    #data[textcol] = stem(data[textcol])\n",
    "\n",
    "    ####################################################################################\n",
    "    #Excluindo variavel categorica\n",
    "    ####################################################################################\n",
    "    \n",
    "    #data = data.drop(['categorica'], axis = 1)\n",
    "                      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale das Variaveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicMeanSd(data, variables):\n",
    "    \n",
    "    dic = {}\n",
    "\n",
    "    for v in variables:\n",
    "\n",
    "        if bool(dic) == False:\n",
    "            dic = {'mean_' + v: data[v].mean()}\n",
    "        if bool(dic) == True:\n",
    "            dic['mean_' + v] = data[v].mean()\n",
    "\n",
    "        dic['sd_' + v] = data[v].std()\n",
    "\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scale(data, variables, dic):\n",
    "    for x in variables:\n",
    "        data[x] = (data[x] - dic['mean_' + x]) / dic['sd_' + x]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lista de Nomes, Averbios, Palavras a Retirar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nomes\n",
    "nomes = pd.read_csv(pathfixo + 'Nomes IBGE.csv', encoding = 'latin1')\n",
    "nomes2 = nomes['Nomes'].apply(lambda x: x.lower()).apply(rem_acentos).unique()\n",
    "\n",
    "#Nomes\n",
    "sobrenomes = pd.read_csv(pathfixo + 'Sobrenomes.csv', encoding = 'latin1')\n",
    "sobrenomes2 = sobrenomes['Sobrenomes'].apply(lambda x: x.lower()).apply(rem_acentos).unique()\n",
    "\n",
    "#Adverbios\n",
    "adverbios = pd.read_csv(pathfixo + 'Adverbios.csv', encoding = 'latin1')\n",
    "adverbios2 = adverbios['Adverbios'].apply(lambda x: x.lower()).apply(rem_acentos).unique()\n",
    "\n",
    "#Palavras adicionais\n",
    "outras = pd.read_csv(pathfixo + 'Outras.csv', encoding = 'latin1')\n",
    "outras2 = outras['Palavras'].apply(lambda x: x.lower()).apply(rem_acentos).unique()\n",
    "\n",
    "#Stopwords\n",
    "#stop = pd.DataFrame({'stop': stopwords.words('portuguese')})\n",
    "stop = pd.DataFrame({'stop': stopwords.words('english')})\n",
    "\n",
    "#Nomes e Palavras\n",
    "NomesEPalavras = np.append(stop, adverbios2)\n",
    "NomesEPalavras = np.append(NomesEPalavras, outras2)\n",
    "#NomesEPalavras = np.append(NomesEPalavras, nomes2)\n",
    "#NomesEPalavras = np.append(NomesEPalavras, sobrenomes2)\n",
    "\n",
    "NomesEPalavras = np.unique(NomesEPalavras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pathaux + 'Functions.pickle', 'wb') as f:\n",
    "    dill.dump((rem_acentos, stem, limpa_nomes, PreProcess, dicMeanSd, Scale, NomesEPalavras), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
